{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "cv_covid_25gb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-yw1TsAeMVt",
        "colab_type": "code",
        "outputId": "c9152bee-c68c-4701-d0d1-abc51588a757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/CVAssignment2/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDpOt3N_jB6O",
        "colab_type": "code",
        "outputId": "8105c6f5-22cd-4234-a2dc-443b59f6fac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "train_x = np.load(root_path+\"Pawan/Split/train_x.npy\")\n",
        "train_y = np.load(root_path+\"Pawan/Split/train_y.npy\")\n",
        "test_x = np.load(root_path+\"Pawan/Split/test_x.npy\")\n",
        "test_y = np.load(root_path+\"Pawan/Split/test_y.npy\")\n",
        "\n",
        "\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1854, 512, 512, 3)\n",
            "(1854, 3)\n",
            "(464, 512, 512, 3)\n",
            "(464, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXBSrM6ENH9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model Architecture\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import backend\n",
        "from keras import models\n",
        "\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, Conv2DTranspose, Activation, add, multiply,Lambda, UpSampling2D, concatenate\n",
        "import os\n",
        "from keras import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications import imagenet_utils\n",
        "from keras import layers\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    weights = K.variable(weights)\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def AttnGatingBlock(x, g, inter_shape, name):\n",
        "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
        "    number of feature channels as x (bigger spatially)\n",
        "    do a conv on x to also get same geature channels (theta_x)\n",
        "    then, upsample g to be same size as x \n",
        "    add x and g (concat_xg)\n",
        "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
        "    \n",
        "    shape_x = K.int_shape(x)  # 32\n",
        "    shape_g = K.int_shape(g)  # 16\n",
        "\n",
        "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl'+name)(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
        "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n",
        "\n",
        "    concat_xg = add([upsample_g, theta_x])\n",
        "    act_xg = Activation('relu')(concat_xg)\n",
        "    psi = Conv2D(1, (1, 1), padding='same', name='psi'+name)(act_xg)\n",
        "    sigmoid_xg = Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = expend_as(upsample_psi, shape_x[3],  name)\n",
        "    y = multiply([upsample_psi, x], name='q_attn'+name)\n",
        "\n",
        "    result = Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n",
        "    result_bn = BatchNormalization(name='q_attn_bn'+name)(result)\n",
        "    return result_bn\n",
        "\n",
        "def UnetGatingSignal(input, is_batchnorm, name):\n",
        "    ''' this is simply 1x1 convolution, bn, activation '''\n",
        "    shape = K.int_shape(input)\n",
        "    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\",  kernel_initializer='glorot_uniform', name=name + '_conv')(input)\n",
        "    if is_batchnorm:\n",
        "        x = BatchNormalization(name=name + '_bn')(x)\n",
        "    x = Activation('relu', name = name + '_act')(x)\n",
        "    return x\n",
        "\n",
        "def expend_as(tensor, rep,name):\n",
        "    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep},  name='psi_up'+name)(tensor)\n",
        "    return my_repeat\n",
        "\n",
        "\n",
        "def dense_layer(y, blocks, name):\n",
        "    for i in range(blocks):\n",
        "        y = conv_layer(y, 32, name=name + '_block' + str(i + 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "def transition_layer(y, reduction, name):\n",
        "    bn_axis = 3 \n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_bn')(y)\n",
        "    y = layers.Activation('relu', name=name + '_relu')(y)\n",
        "    y = layers.Conv2D(int(backend.int_shape(y)[bn_axis] * reduction), 1, use_bias=False, name=name + '_conv')(y)\n",
        "    y = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "def conv_layer(y, growth_rate, name):\n",
        "    bn_axis = 3 \n",
        "    y1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(y)\n",
        "    y1 = layers.Activation('relu', name=name + '_0_relu')(y1)\n",
        "    y1 = layers.SeparableConv2D(4 * growth_rate, 1, use_bias=False, name=name + '_1_conv')(y1)\n",
        "    y1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(y1)\n",
        "    y1 = layers.Activation('relu', name=name + '_1_relu')(y1)\n",
        "    y1 = layers.SeparableConv2D(growth_rate, 3, padding='same', use_bias=False, name=name + '_2_conv')(y1)\n",
        "    y = layers.Concatenate(axis=bn_axis, name=name + '_concatenate')([y, y1])\n",
        "    return y\n",
        "\n",
        "def Model1():\n",
        "    bn_axis = 3\n",
        "\n",
        "    image=layers.Input((512,512,3))\n",
        "    y = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(image)\n",
        "    y = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(y)\n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(y)\n",
        "    y = layers.Activation('relu', name='conv1/relu')(y)\n",
        "    y = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(y)\n",
        "    y = layers.MaxPooling2D(3, strides=2, name='pool1')(y)\n",
        "\n",
        "    fire2_squeeze = Conv2D(16, 1, 1, activation='relu', init='glorot_uniform',border_mode='same', name='fire2_squeeze')(y)\n",
        "    fire2_expand1 = Conv2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same', name='fire2_expand1')(fire2_squeeze)\n",
        "    fire2_expand2 = Conv2D(64, 3, 3, activation='relu', init='glorot_uniform',border_mode='same', name='fire2_expand2')(fire2_squeeze)\n",
        "    merge = layers.Concatenate(axis=bn_axis)([fire2_expand1, fire2_expand2])\n",
        "    merge = layers.Concatenate(axis=bn_axis)([merge, y])\n",
        "\n",
        "    y = dense_layer(merge, 6, name='conv2')\n",
        "    y = transition_layer(y, 0.5, name='pool2')\n",
        "    y = dense_layer(y, 12, name='conv3')\n",
        "    y = transition_layer(y, 0.5, name='pool3')\n",
        "    y = dense_layer(y, 24, name='conv4')\n",
        "    y = transition_layer(y, 0.5, name='pool4')\n",
        "    y = dense_layer(y, 16, name='conv5')\n",
        "\n",
        "    fire2_squeeze = Conv2D(64, 1, 1, activation='relu', init='glorot_uniform',border_mode='same', name='firen_squeeze')(y)\n",
        "    fire2_expand1 = Conv2D(256, 1, 1, activation='relu', init='glorot_uniform',border_mode='same', name='firen_expand1')(fire2_squeeze)\n",
        "    fire2_expand2 = Conv2D(256, 3, 3, activation='relu', init='glorot_uniform',border_mode='same', name='firen_expand2')(fire2_squeeze)\n",
        "    merge = layers.Concatenate(axis=bn_axis)([fire2_expand1, fire2_expand2])\n",
        "    merge = layers.Concatenate(axis=bn_axis)([merge, y])\n",
        "\n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(merge)\n",
        "    y = layers.Activation('relu', name='relu')(y)\n",
        "    y2 = GlobalAveragePooling2D()(y)\n",
        "    y3 = Dropout(0.3)(y2)\n",
        "    output1 = Dense(10,activation='relu')(y3)\n",
        "    output2 = Dense(3,activation='softmax')(output1)\n",
        "\n",
        "    model = models.Model( image, output2, name='model_1')\n",
        "    return model\n",
        "\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "model=Model1()\n",
        "model.compile(optimizer=Adam(lr=3e-4), loss=weighted_categorical_crossentropy([0.1723,1.0,1.0117]), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG_ggWEdabTt",
        "colab_type": "code",
        "outputId": "9497a044-eec8-4d07-c0e5-27b337ea4d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model Architecture\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import backend\n",
        "from keras import models\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, Conv2DTranspose, Activation, add, multiply,Lambda, UpSampling2D, concatenate\n",
        "import os\n",
        "from keras import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications import imagenet_utils\n",
        "from keras import layers\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    weights = K.variable(weights)\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def AttnGatingBlock(x, g, inter_shape, name):\n",
        "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
        "    number of feature channels as x (bigger spatially)\n",
        "    do a conv on x to also get same geature channels (theta_x)\n",
        "    then, upsample g to be same size as x \n",
        "    add x and g (concat_xg)\n",
        "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
        "    \n",
        "    shape_x = K.int_shape(x)  # 32\n",
        "    shape_g = K.int_shape(g)  # 16\n",
        "\n",
        "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl'+name)(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
        "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n",
        "\n",
        "    concat_xg = add([upsample_g, theta_x])\n",
        "    act_xg = Activation('relu')(concat_xg)\n",
        "    psi = Conv2D(1, (1, 1), padding='same', name='psi'+name)(act_xg)\n",
        "    sigmoid_xg = Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = expend_as(upsample_psi, shape_x[3],  name)\n",
        "    y = multiply([upsample_psi, x], name='q_attn'+name)\n",
        "\n",
        "    result = Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n",
        "    result_bn = BatchNormalization(name='q_attn_bn'+name)(result)\n",
        "    return result_bn\n",
        "\n",
        "def UnetGatingSignal(input, is_batchnorm, name):\n",
        "    ''' this is simply 1x1 convolution, bn, activation '''\n",
        "    shape = K.int_shape(input)\n",
        "    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\",  kernel_initializer='glorot_uniform', name=name + '_conv')(input)\n",
        "    if is_batchnorm:\n",
        "        x = BatchNormalization(name=name + '_bn')(x)\n",
        "    x = Activation('relu', name = name + '_act')(x)\n",
        "    return x\n",
        "\n",
        "def expend_as(tensor, rep,name):\n",
        "    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep},  name='psi_up'+name)(tensor)\n",
        "    return my_repeat\n",
        "\n",
        "\n",
        "def dense_layer(y, blocks, name):\n",
        "    for i in range(blocks):\n",
        "        y = conv_layer(y, 32, name=name + '_block' + str(i + 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "def transition_layer(y, reduction, name):\n",
        "    bn_axis = 3 \n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_bn')(y)\n",
        "    y = layers.Activation('relu', name=name + '_relu')(y)\n",
        "    y = layers.Conv2D(int(backend.int_shape(y)[bn_axis] * reduction), 1, use_bias=False, name=name + '_conv')(y)\n",
        "    y = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "def conv_layer(y, growth_rate, name):\n",
        "    bn_axis = 3 \n",
        "    y1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(y)\n",
        "    y1 = layers.Activation('relu', name=name + '_0_relu')(y1)\n",
        "    y1 = layers.SeparableConv2D(4 * growth_rate, 1, use_bias=False, name=name + '_1_conv')(y1)\n",
        "    y1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(y1)\n",
        "    y1 = layers.Activation('relu', name=name + '_1_relu')(y1)\n",
        "    y1 = layers.SeparableConv2D(growth_rate, 3, padding='same', use_bias=False, name=name + '_2_conv')(y1)\n",
        "    y = layers.Concatenate(axis=bn_axis, name=name + '_concatenate')([y, y1])\n",
        "    return y\n",
        "\n",
        "def Model1():\n",
        "    bn_axis = 3\n",
        "\n",
        "    image=layers.Input((512,512,3))\n",
        "    y = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(image)\n",
        "    y = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(y)\n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(y)\n",
        "    y = layers.Activation('relu', name='conv1/relu')(y)\n",
        "    y = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(y)\n",
        "    y = layers.MaxPooling2D(3, strides=2, name='pool1')(y)\n",
        "\n",
        "    y = dense_layer(y, 4, name='conv2')\n",
        "    y = transition_layer(y, 0.5, name='pool2')\n",
        "    y1 = dense_layer(y, 8, name='conv3')\n",
        "    y = transition_layer(y1, 0.5, name='pool3')\n",
        "    g2 = UnetGatingSignal(y, is_batchnorm=True, name='g2')\n",
        "    attn2 = AttnGatingBlock(y1, g2, 128, '_2')\n",
        "    y = concatenate([Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer='glorot_uniform')(y), attn2], name='up2')\n",
        "    y = dense_layer(y, 16, name='conv4')\n",
        "    y = transition_layer(y, 0.5, name='pool4')\n",
        "    y = dense_layer(y, 12, name='conv5')\n",
        "\n",
        "    y = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(y)\n",
        "    y = layers.Activation('relu', name='relu')(y)\n",
        "    y2 = GlobalAveragePooling2D()(y)\n",
        "    y3 = Dropout(0.3)(y2)\n",
        "    output1 = Dense(10,activation='relu')(y3)\n",
        "    output2 = Dense(3,activation='softmax')(output1)\n",
        "\n",
        "    model = models.Model(image , output2, name='model_1')\n",
        "    return model\n",
        "\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "model=Model1()\n",
        "model.compile(optimizer=Adam(lr=3e-4), loss=weighted_categorical_crossentropy([0.1723,1.0,1.0117]), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 518, 518, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 256, 256, 64) 9408        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 256, 256, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 258, 258, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 128, 128, 64) 0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 128, 128, 64) 256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 128, 128, 64) 0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (SeparableC (None, 128, 128, 128 8256        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 128, 128, 128 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (SeparableC (None, 128, 128, 32) 5248        conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concatenate (Conca (None, 128, 128, 96) 0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 128, 128, 96) 384         conv2_block1_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 128, 128, 96) 0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (SeparableC (None, 128, 128, 128 12384       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 128, 128, 128 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (SeparableC (None, 128, 128, 32) 5248        conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concatenate (Conca (None, 128, 128, 128 0           conv2_block1_concatenate[0][0]   \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 128, 128, 128 0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (SeparableC (None, 128, 128, 128 16512       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 128, 128, 128 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (SeparableC (None, 128, 128, 32) 5248        conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concatenate (Conca (None, 128, 128, 160 0           conv2_block2_concatenate[0][0]   \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 128, 128, 160 640         conv2_block3_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 128, 128, 160 0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (SeparableC (None, 128, 128, 128 20640       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 128, 128, 128 0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (SeparableC (None, 128, 128, 32) 5248        conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concatenate (Conca (None, 128, 128, 192 0           conv2_block3_concatenate[0][0]   \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 128, 128, 192 768         conv2_block4_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 128, 128, 192 0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 128, 128, 96) 18432       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 64, 64, 96)   0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 64, 64, 96)   384         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 64, 64, 96)   0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (SeparableC (None, 64, 64, 128)  12384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concatenate (Conca (None, 64, 64, 128)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 64, 64, 128)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (SeparableC (None, 64, 64, 128)  16512       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concatenate (Conca (None, 64, 64, 160)  0           conv3_block1_concatenate[0][0]   \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 64, 64, 160)  640         conv3_block2_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 64, 64, 160)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (SeparableC (None, 64, 64, 128)  20640       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concatenate (Conca (None, 64, 64, 192)  0           conv3_block2_concatenate[0][0]   \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 64, 64, 192)  768         conv3_block3_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 64, 64, 192)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (SeparableC (None, 64, 64, 128)  24768       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concatenate (Conca (None, 64, 64, 224)  0           conv3_block3_concatenate[0][0]   \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 64, 64, 224)  896         conv3_block4_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 64, 64, 224)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (SeparableC (None, 64, 64, 128)  28896       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 64, 64, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concatenate (Conca (None, 64, 64, 256)  0           conv3_block4_concatenate[0][0]   \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv3_block5_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 64, 64, 256)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (SeparableC (None, 64, 64, 128)  33024       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 64, 64, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concatenate (Conca (None, 64, 64, 288)  0           conv3_block5_concatenate[0][0]   \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 64, 64, 288)  1152        conv3_block6_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 64, 64, 288)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (SeparableC (None, 64, 64, 128)  37152       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 64, 64, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concatenate (Conca (None, 64, 64, 320)  0           conv3_block6_concatenate[0][0]   \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 64, 64, 320)  1280        conv3_block7_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 64, 64, 320)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (SeparableC (None, 64, 64, 128)  41280       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 64, 64, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (SeparableC (None, 64, 64, 32)   5248        conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concatenate (Conca (None, 64, 64, 352)  0           conv3_block7_concatenate[0][0]   \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 64, 64, 352)  1408        conv3_block8_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 64, 64, 352)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 64, 64, 176)  61952       pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 32, 32, 176)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "g2_conv (Conv2D)                (None, 32, 32, 176)  31152       pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "g2_bn (BatchNormalization)      (None, 32, 32, 176)  704         g2_conv[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "g2_act (Activation)             (None, 32, 32, 176)  0           g2_bn[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  22656       g2_act[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "g_up_2 (Conv2DTranspose)        (None, 32, 32, 128)  147584      conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "xl_2 (Conv2D)                   (None, 32, 32, 128)  180352      conv3_block8_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 128)  0           g_up_2[0][0]                     \n",
            "                                                                 xl_2[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "psi_2 (Conv2D)                  (None, 32, 32, 1)    129         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 1)    0           psi_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 1)    0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "psi_up_2 (Lambda)               (None, 64, 64, 352)  0           up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "q_attn_2 (Multiply)             (None, 64, 64, 352)  0           psi_up_2[0][0]                   \n",
            "                                                                 conv3_block8_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "q_attn_conv_2 (Conv2D)          (None, 64, 64, 352)  124256      q_attn_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  202880      pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "q_attn_bn_2 (BatchNormalization (None, 64, 64, 352)  1408        q_attn_conv_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up2 (Concatenate)               (None, 64, 64, 480)  0           conv2d_transpose_1[0][0]         \n",
            "                                                                 q_attn_bn_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 64, 64, 480)  1920        up2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 64, 64, 480)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (SeparableC (None, 64, 64, 128)  61920       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 64, 64, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concatenate (Conca (None, 64, 64, 512)  0           up2[0][0]                        \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 64, 64, 512)  2048        conv4_block1_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 64, 64, 512)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (SeparableC (None, 64, 64, 128)  66048       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 64, 64, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concatenate (Conca (None, 64, 64, 544)  0           conv4_block1_concatenate[0][0]   \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 64, 64, 544)  2176        conv4_block2_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 64, 64, 544)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (SeparableC (None, 64, 64, 128)  70176       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 64, 64, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concatenate (Conca (None, 64, 64, 576)  0           conv4_block2_concatenate[0][0]   \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 64, 64, 576)  2304        conv4_block3_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 64, 64, 576)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (SeparableC (None, 64, 64, 128)  74304       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 64, 64, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concatenate (Conca (None, 64, 64, 608)  0           conv4_block3_concatenate[0][0]   \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 64, 64, 608)  2432        conv4_block4_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 64, 64, 608)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (SeparableC (None, 64, 64, 128)  78432       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 64, 64, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concatenate (Conca (None, 64, 64, 640)  0           conv4_block4_concatenate[0][0]   \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 64, 64, 640)  2560        conv4_block5_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 64, 64, 640)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (SeparableC (None, 64, 64, 128)  82560       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 64, 64, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concatenate (Conca (None, 64, 64, 672)  0           conv4_block5_concatenate[0][0]   \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 64, 64, 672)  2688        conv4_block6_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 64, 64, 672)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (SeparableC (None, 64, 64, 128)  86688       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 64, 64, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concatenate (Conca (None, 64, 64, 704)  0           conv4_block6_concatenate[0][0]   \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 64, 64, 704)  2816        conv4_block7_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 64, 64, 704)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (SeparableC (None, 64, 64, 128)  90816       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 64, 64, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concatenate (Conca (None, 64, 64, 736)  0           conv4_block7_concatenate[0][0]   \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 64, 64, 736)  2944        conv4_block8_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 64, 64, 736)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (SeparableC (None, 64, 64, 128)  94944       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 64, 64, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 64, 64, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (SeparableC (None, 64, 64, 32)   5248        conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concatenate (Conca (None, 64, 64, 768)  0           conv4_block8_concatenate[0][0]   \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 64, 64, 768)  3072        conv4_block9_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 64, 64, 768)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Separable (None, 64, 64, 128)  99072       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concatenate (Conc (None, 64, 64, 800)  0           conv4_block9_concatenate[0][0]   \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 64, 64, 800)  3200        conv4_block10_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 64, 64, 800)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Separable (None, 64, 64, 128)  103200      conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concatenate (Conc (None, 64, 64, 832)  0           conv4_block10_concatenate[0][0]  \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 64, 64, 832)  3328        conv4_block11_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 64, 64, 832)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Separable (None, 64, 64, 128)  107328      conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concatenate (Conc (None, 64, 64, 864)  0           conv4_block11_concatenate[0][0]  \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 64, 64, 864)  3456        conv4_block12_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 64, 64, 864)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Separable (None, 64, 64, 128)  111456      conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concatenate (Conc (None, 64, 64, 896)  0           conv4_block12_concatenate[0][0]  \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 64, 64, 896)  3584        conv4_block13_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 64, 64, 896)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Separable (None, 64, 64, 128)  115584      conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concatenate (Conc (None, 64, 64, 928)  0           conv4_block13_concatenate[0][0]  \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 64, 64, 928)  3712        conv4_block14_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 64, 64, 928)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Separable (None, 64, 64, 128)  119712      conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concatenate (Conc (None, 64, 64, 960)  0           conv4_block14_concatenate[0][0]  \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 64, 64, 960)  3840        conv4_block15_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 64, 64, 960)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Separable (None, 64, 64, 128)  123840      conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 64, 64, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 64, 64, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Separable (None, 64, 64, 32)   5248        conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concatenate (Conc (None, 64, 64, 992)  0           conv4_block15_concatenate[0][0]  \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 64, 64, 992)  3968        conv4_block16_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 64, 64, 992)  0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 64, 64, 496)  492032      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 32, 32, 496)  0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 32, 32, 496)  1984        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 32, 32, 496)  0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (SeparableC (None, 32, 32, 128)  63984       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 32, 32, 128)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concatenate (Conca (None, 32, 32, 528)  0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 32, 32, 528)  2112        conv5_block1_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 32, 32, 528)  0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (SeparableC (None, 32, 32, 128)  68112       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 32, 32, 128)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concatenate (Conca (None, 32, 32, 560)  0           conv5_block1_concatenate[0][0]   \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 32, 32, 560)  2240        conv5_block2_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 32, 32, 560)  0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (SeparableC (None, 32, 32, 128)  72240       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 32, 32, 128)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concatenate (Conca (None, 32, 32, 592)  0           conv5_block2_concatenate[0][0]   \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 32, 32, 592)  2368        conv5_block3_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 32, 32, 592)  0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (SeparableC (None, 32, 32, 128)  76368       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 32, 32, 128)  0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concatenate (Conca (None, 32, 32, 624)  0           conv5_block3_concatenate[0][0]   \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 32, 32, 624)  2496        conv5_block4_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 32, 32, 624)  0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (SeparableC (None, 32, 32, 128)  80496       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 32, 32, 128)  0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concatenate (Conca (None, 32, 32, 656)  0           conv5_block4_concatenate[0][0]   \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 32, 32, 656)  2624        conv5_block5_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 32, 32, 656)  0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (SeparableC (None, 32, 32, 128)  84624       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 32, 32, 128)  0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concatenate (Conca (None, 32, 32, 688)  0           conv5_block5_concatenate[0][0]   \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 32, 32, 688)  2752        conv5_block6_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 32, 32, 688)  0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (SeparableC (None, 32, 32, 128)  88752       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 32, 32, 128)  0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concatenate (Conca (None, 32, 32, 720)  0           conv5_block6_concatenate[0][0]   \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 32, 32, 720)  2880        conv5_block7_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 32, 32, 720)  0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (SeparableC (None, 32, 32, 128)  92880       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 32, 32, 128)  0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concatenate (Conca (None, 32, 32, 752)  0           conv5_block7_concatenate[0][0]   \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 32, 32, 752)  3008        conv5_block8_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 32, 32, 752)  0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (SeparableC (None, 32, 32, 128)  97008       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 32, 32, 128)  512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 32, 32, 128)  0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (SeparableC (None, 32, 32, 32)   5248        conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concatenate (Conca (None, 32, 32, 784)  0           conv5_block8_concatenate[0][0]   \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 32, 32, 784)  3136        conv5_block9_concatenate[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 32, 32, 784)  0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Separable (None, 32, 32, 128)  101136      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 32, 32, 128)  512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 32, 32, 128)  0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Separable (None, 32, 32, 32)   5248        conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concatenate (Conc (None, 32, 32, 816)  0           conv5_block9_concatenate[0][0]   \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 32, 32, 816)  3264        conv5_block10_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 32, 32, 816)  0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Separable (None, 32, 32, 128)  105264      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 32, 32, 128)  512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 32, 32, 128)  0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Separable (None, 32, 32, 32)   5248        conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concatenate (Conc (None, 32, 32, 848)  0           conv5_block10_concatenate[0][0]  \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 32, 32, 848)  3392        conv5_block11_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 32, 32, 848)  0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Separable (None, 32, 32, 128)  109392      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 32, 32, 128)  512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 32, 32, 128)  0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Separable (None, 32, 32, 32)   5248        conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concatenate (Conc (None, 32, 32, 880)  0           conv5_block11_concatenate[0][0]  \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 32, 32, 880)  3520        conv5_block12_concatenate[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 32, 32, 880)  0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 880)          0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 880)          0           global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           8810        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3)            33          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,427,676\n",
            "Trainable params: 4,368,028\n",
            "Non-trainable params: 59,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8kmO2wXEaex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import math\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,CSVLogger\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "mc = ModelCheckpoint(root_path+'Logs/rbg_best0007.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "cv = CSVLogger(root_path+'Logs/rbg_best0007.csv', separator=',', append=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fnhBhqjFH6F",
        "colab_type": "code",
        "outputId": "edd8adc4-91b8-4684-9a9b-e8518d70a4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_x, train_y, batch_size=8, epochs=100, validation_split=(0.2),verbose=1,callbacks=[es,mc,cv])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1483 samples, validate on 371 samples\n",
            "Epoch 1/100\n",
            "1483/1483 [==============================] - 271s 183ms/step - loss: 0.4422 - accuracy: 0.7782 - val_loss: 0.9940 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.99397, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 2/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.3065 - accuracy: 0.8348 - val_loss: 1.2396 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.99397\n",
            "Epoch 3/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.2987 - accuracy: 0.8456 - val_loss: 1.1633 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.99397\n",
            "Epoch 4/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.2485 - accuracy: 0.8550 - val_loss: 7.6185 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.99397\n",
            "Epoch 5/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.2093 - accuracy: 0.8645 - val_loss: 7.6312 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.99397\n",
            "Epoch 6/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.2245 - accuracy: 0.8564 - val_loss: 0.8842 - val_accuracy: 0.6631\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.99397 to 0.88415, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 7/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1942 - accuracy: 0.8732 - val_loss: 0.9865 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.88415\n",
            "Epoch 8/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1979 - accuracy: 0.8726 - val_loss: 1.4356 - val_accuracy: 0.4744\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.88415\n",
            "Epoch 9/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1675 - accuracy: 0.8840 - val_loss: 5.0974 - val_accuracy: 0.4555\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.88415\n",
            "Epoch 10/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1573 - accuracy: 0.9009 - val_loss: 5.2117 - val_accuracy: 0.0701\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.88415\n",
            "Epoch 11/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1802 - accuracy: 0.8975 - val_loss: 7.6312 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.88415\n",
            "Epoch 12/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1382 - accuracy: 0.9171 - val_loss: 11.0640 - val_accuracy: 0.0674\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.88415\n",
            "Epoch 13/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1412 - accuracy: 0.9164 - val_loss: 3.3415 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.88415\n",
            "Epoch 14/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1472 - accuracy: 0.9204 - val_loss: 1.8746 - val_accuracy: 0.4771\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.88415\n",
            "Epoch 15/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1205 - accuracy: 0.9319 - val_loss: 0.5506 - val_accuracy: 0.7385\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.88415 to 0.55064, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 16/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1029 - accuracy: 0.9400 - val_loss: 13.0927 - val_accuracy: 0.1213\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.55064\n",
            "Epoch 17/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1191 - accuracy: 0.9312 - val_loss: 2.4361 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.55064\n",
            "Epoch 18/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1314 - accuracy: 0.9299 - val_loss: 9.5888 - val_accuracy: 0.0728\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.55064\n",
            "Epoch 19/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1186 - accuracy: 0.9326 - val_loss: 0.3174 - val_accuracy: 0.8464\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.55064 to 0.31745, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 20/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1047 - accuracy: 0.9467 - val_loss: 3.7013 - val_accuracy: 0.4771\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.31745\n",
            "Epoch 21/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1250 - accuracy: 0.9346 - val_loss: 7.2345 - val_accuracy: 0.4690\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.31745\n",
            "Epoch 22/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0963 - accuracy: 0.9447 - val_loss: 5.5925 - val_accuracy: 0.2183\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.31745\n",
            "Epoch 23/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1039 - accuracy: 0.9454 - val_loss: 7.2942 - val_accuracy: 0.5013\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.31745\n",
            "Epoch 24/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1093 - accuracy: 0.9454 - val_loss: 0.1964 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.31745 to 0.19638, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 25/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1239 - accuracy: 0.9420 - val_loss: 5.8511 - val_accuracy: 0.2803\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.19638\n",
            "Epoch 26/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0754 - accuracy: 0.9562 - val_loss: 0.4259 - val_accuracy: 0.8248\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.19638\n",
            "Epoch 27/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1020 - accuracy: 0.9501 - val_loss: 0.4850 - val_accuracy: 0.8437\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.19638\n",
            "Epoch 28/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0724 - accuracy: 0.9589 - val_loss: 0.5554 - val_accuracy: 0.7844\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.19638\n",
            "Epoch 29/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0727 - accuracy: 0.9568 - val_loss: 4.6815 - val_accuracy: 0.4906\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.19638\n",
            "Epoch 30/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0599 - accuracy: 0.9656 - val_loss: 0.2692 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.19638\n",
            "Epoch 31/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0957 - accuracy: 0.9595 - val_loss: 5.4494 - val_accuracy: 0.0943\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.19638\n",
            "Epoch 32/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0719 - accuracy: 0.9494 - val_loss: 7.7393 - val_accuracy: 0.4178\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.19638\n",
            "Epoch 33/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.1017 - accuracy: 0.9481 - val_loss: 3.1270 - val_accuracy: 0.4717\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.19638\n",
            "Epoch 34/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0927 - accuracy: 0.9541 - val_loss: 0.3144 - val_accuracy: 0.8706\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.19638\n",
            "Epoch 35/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0703 - accuracy: 0.9582 - val_loss: 1.5384 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.19638\n",
            "Epoch 36/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0504 - accuracy: 0.9676 - val_loss: 3.9161 - val_accuracy: 0.1429\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.19638\n",
            "Epoch 37/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0520 - accuracy: 0.9764 - val_loss: 9.8889 - val_accuracy: 0.0728\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.19638\n",
            "Epoch 38/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0576 - accuracy: 0.9697 - val_loss: 0.4687 - val_accuracy: 0.8491\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.19638\n",
            "Epoch 39/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0362 - accuracy: 0.9764 - val_loss: 1.9887 - val_accuracy: 0.5364\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.19638\n",
            "Epoch 40/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0581 - accuracy: 0.9676 - val_loss: 0.3159 - val_accuracy: 0.8625\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.19638\n",
            "Epoch 41/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0416 - accuracy: 0.9751 - val_loss: 9.8231 - val_accuracy: 0.0997\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.19638\n",
            "Epoch 42/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0389 - accuracy: 0.9744 - val_loss: 0.4222 - val_accuracy: 0.8005\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.19638\n",
            "Epoch 43/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0430 - accuracy: 0.9771 - val_loss: 0.4982 - val_accuracy: 0.8329\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.19638\n",
            "Epoch 44/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0530 - accuracy: 0.9744 - val_loss: 0.1648 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.19638 to 0.16483, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 45/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0383 - accuracy: 0.9784 - val_loss: 0.8036 - val_accuracy: 0.7844\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.16483\n",
            "Epoch 46/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0363 - accuracy: 0.9791 - val_loss: 0.5488 - val_accuracy: 0.8059\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.16483\n",
            "Epoch 47/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0538 - accuracy: 0.9751 - val_loss: 1.6933 - val_accuracy: 0.3908\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.16483\n",
            "Epoch 48/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0662 - accuracy: 0.9717 - val_loss: 0.6319 - val_accuracy: 0.8086\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.16483\n",
            "Epoch 49/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0501 - accuracy: 0.9730 - val_loss: 2.0088 - val_accuracy: 0.4043\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.16483\n",
            "Epoch 50/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0388 - accuracy: 0.9791 - val_loss: 0.2954 - val_accuracy: 0.8787\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.16483\n",
            "Epoch 51/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0554 - accuracy: 0.9683 - val_loss: 12.2717 - val_accuracy: 0.0728\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.16483\n",
            "Epoch 52/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0370 - accuracy: 0.9744 - val_loss: 0.1664 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.16483\n",
            "Epoch 53/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0291 - accuracy: 0.9858 - val_loss: 0.4935 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.16483\n",
            "Epoch 54/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0132 - accuracy: 0.9939 - val_loss: 0.4499 - val_accuracy: 0.8652\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.16483\n",
            "Epoch 55/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0296 - accuracy: 0.9825 - val_loss: 3.6615 - val_accuracy: 0.1644\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.16483\n",
            "Epoch 56/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0534 - accuracy: 0.9676 - val_loss: 2.0196 - val_accuracy: 0.5741\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.16483\n",
            "Epoch 57/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0680 - accuracy: 0.9562 - val_loss: 3.3600 - val_accuracy: 0.5121\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.16483\n",
            "Epoch 58/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0612 - accuracy: 0.9643 - val_loss: 0.1883 - val_accuracy: 0.9326\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.16483\n",
            "Epoch 59/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0438 - accuracy: 0.9764 - val_loss: 0.1934 - val_accuracy: 0.9245\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.16483\n",
            "Epoch 60/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0176 - accuracy: 0.9885 - val_loss: 0.1431 - val_accuracy: 0.9326\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.16483 to 0.14310, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 61/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0228 - accuracy: 0.9879 - val_loss: 0.2424 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.14310\n",
            "Epoch 62/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0228 - accuracy: 0.9858 - val_loss: 0.2793 - val_accuracy: 0.9137\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.14310\n",
            "Epoch 63/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0340 - accuracy: 0.9838 - val_loss: 0.3561 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.14310\n",
            "Epoch 64/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.2589 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.14310\n",
            "Epoch 65/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0317 - accuracy: 0.9831 - val_loss: 0.2524 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.14310\n",
            "Epoch 66/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0116 - accuracy: 0.9906 - val_loss: 0.2472 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.14310\n",
            "Epoch 67/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0278 - accuracy: 0.9825 - val_loss: 0.3335 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.14310\n",
            "Epoch 68/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0233 - accuracy: 0.9865 - val_loss: 0.5746 - val_accuracy: 0.8329\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.14310\n",
            "Epoch 69/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0207 - accuracy: 0.9899 - val_loss: 0.3228 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.14310\n",
            "Epoch 70/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0192 - accuracy: 0.9879 - val_loss: 0.5747 - val_accuracy: 0.7978\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.14310\n",
            "Epoch 71/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0291 - accuracy: 0.9879 - val_loss: 0.3244 - val_accuracy: 0.8787\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.14310\n",
            "Epoch 72/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0488 - accuracy: 0.9690 - val_loss: 1.5949 - val_accuracy: 0.6954\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.14310\n",
            "Epoch 73/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0224 - accuracy: 0.9858 - val_loss: 0.2776 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.14310\n",
            "Epoch 74/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0120 - accuracy: 0.9926 - val_loss: 0.1539 - val_accuracy: 0.9461\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.14310\n",
            "Epoch 75/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0265 - accuracy: 0.9872 - val_loss: 0.7759 - val_accuracy: 0.7385\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.14310\n",
            "Epoch 76/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0298 - accuracy: 0.9865 - val_loss: 0.4674 - val_accuracy: 0.8464\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.14310\n",
            "Epoch 77/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0264 - accuracy: 0.9865 - val_loss: 0.3565 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.14310\n",
            "Epoch 78/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0162 - accuracy: 0.9879 - val_loss: 0.1372 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.14310 to 0.13725, saving model to gdrive/My Drive/CVAssignment2/Logs/rbg_best0007.h5\n",
            "Epoch 79/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0481 - accuracy: 0.9784 - val_loss: 0.5424 - val_accuracy: 0.8706\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.13725\n",
            "Epoch 80/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0201 - accuracy: 0.9865 - val_loss: 0.3431 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.13725\n",
            "Epoch 81/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0172 - accuracy: 0.9885 - val_loss: 0.1695 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.13725\n",
            "Epoch 82/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0128 - accuracy: 0.9919 - val_loss: 0.6040 - val_accuracy: 0.8491\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.13725\n",
            "Epoch 83/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0181 - accuracy: 0.9879 - val_loss: 0.1953 - val_accuracy: 0.9488\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.13725\n",
            "Epoch 84/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0415 - accuracy: 0.9791 - val_loss: 1.2982 - val_accuracy: 0.6765\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.13725\n",
            "Epoch 85/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0183 - accuracy: 0.9865 - val_loss: 0.3996 - val_accuracy: 0.8652\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.13725\n",
            "Epoch 86/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0142 - accuracy: 0.9906 - val_loss: 0.9025 - val_accuracy: 0.7574\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.13725\n",
            "Epoch 87/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0413 - accuracy: 0.9818 - val_loss: 13.6195 - val_accuracy: 0.0728\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.13725\n",
            "Epoch 88/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0229 - accuracy: 0.9892 - val_loss: 0.1670 - val_accuracy: 0.9488\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.13725\n",
            "Epoch 89/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0122 - accuracy: 0.9919 - val_loss: 1.0965 - val_accuracy: 0.6981\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.13725\n",
            "Epoch 90/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0265 - accuracy: 0.9858 - val_loss: 0.5661 - val_accuracy: 0.8464\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.13725\n",
            "Epoch 91/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0175 - accuracy: 0.9865 - val_loss: 0.1512 - val_accuracy: 0.9461\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.13725\n",
            "Epoch 92/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0104 - accuracy: 0.9919 - val_loss: 0.1652 - val_accuracy: 0.9515\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.13725\n",
            "Epoch 93/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0096 - accuracy: 0.9939 - val_loss: 0.2126 - val_accuracy: 0.9272\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.13725\n",
            "Epoch 94/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0044 - accuracy: 0.9953 - val_loss: 0.1964 - val_accuracy: 0.9461\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.13725\n",
            "Epoch 95/100\n",
            "1483/1483 [==============================] - 227s 153ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.1561 - val_accuracy: 0.9569\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.13725\n",
            "Epoch 96/100\n",
            " 448/1483 [========>.....................] - ETA: 2:30 - loss: 0.0363 - accuracy: 0.9844"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYAGqLRKFNlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(root_path+'Logs/rbg_last0007.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us_RDTzcFY1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(root_path+'Logs/rbg_best0007.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKyv_Y-AFcRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = model.predict(test_x, batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCOmqQpMFmJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ftest = np.zeros(len(test_pred))\n",
        "atest = np.zeros(len(test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d_Ok6plFoy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_y)):\n",
        "  if test_y[i][0] >= test_y[i][1] and test_y[i][0] >= test_y[i][2]:\n",
        "    atest[i] = 0\n",
        "  elif test_y[i][1] >= test_y[i][0] and test_y[i][1] >= test_y[i][2]:\n",
        "    atest[i] = 1\n",
        "  else:\n",
        "    atest[i] = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWCPQKYuFpe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_pred)):\n",
        "  if test_pred[i][0] >= test_pred[i][1] and test_pred[i][0] >= test_pred[i][2]:\n",
        "    ftest[i] = 0\n",
        "  elif test_pred[i][1] >= test_pred[i][0] and test_pred[i][1] >= test_pred[i][2]:\n",
        "    ftest[i] = 1\n",
        "  else:\n",
        "    ftest[i] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IwlvUGqFpiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(atest, ftest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCZz2X-GFpov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(atest, ftest, digits=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFwDFF6qFps-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5 is attention plus squeeze plus densenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW4RtVVE9LYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6 is squeeze plus densenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlfM0VDCa_dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 7 is attention plus densenet increased values"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}